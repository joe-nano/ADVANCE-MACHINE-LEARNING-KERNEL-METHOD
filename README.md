# ADVANCE-MACHINE-LEARNING-KERNEL-METHOD
Advance machine Learning: Kernel methods implemented for PCA, KMeans, Logistic Regression and SVDD

##### CLASSICAL KMEANS | [code](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/KMEANS/kmeans.py) | KERNEL KMEANS [CODE](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/KMEANS/KERNEL%20KMEANS/kernelkmeans.py) | [Paper](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/REPORT/KMeans.pdf)
##### Kmeans is a simple yet efficient unsupervised clustering algorithm. K-Means clustering is a fast, robust, and simple algorithm that gives reliable results when data sets are distinct or well separated from each other in a linear fashion. It is best used when the number of cluster centers, is specified due to a well-defined list of types shown in the data.

##### CLASSICAL LOGISTIC REGRESSION | [code](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/LOGISTIC%20REGRESSION/losgisticRegression.py) | KERNEL LOGISTIC REGRESSION [CODE](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/LOGISTIC%20REGRESSION/kernellogistic.py) | [Paper](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/REPORT/LogisticRegression.pdf) 
##### Logistic regression solves the limitation of linear regression for categorical variable using maximum likelihood estimation of probability log function. This idea is further explained in the next sections. Our focus however is on its kernel version and how we explore the inner product of the independent variable to classify non-seperable data.

##### CLASSICAL SVDD | [code](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/ONE%20CLASS%20SVM(SVDD)/linearSVDD.py) | KERNEL SVDD [CODE](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/ONE%20CLASS%20SVM(SVDD)/DualSVDD.py) | [Paper](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/REPORT/oneclasssvm.pdf)
##### Support Vector Data Description (SVDD) is a variant of Support Vector Machines (SVM), usually referred to as the One class SVM. It is interesting for use cases where researchers are only interested in the positive class class of interest, therefore making it suitable to detect novel data or outliers.

##### CLASSICAL PCA | [code](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/PCA/PCA.py) | KERNEL PCA [CODE](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/tree/master/PCA/KERNEL%20PCA) | [Paper](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/REPORT/PCA.pdf)
##### Principal Component Analysis (PCA) is an unsupervised dimension reduction technique that depends on the orthogonal transformation of a higher dimensional data space to a lower dimensional subspace. This implicitly means it is used for feature extraction since some of the features in the original space may not be required for projecting the data in the reduced subspace.

##### CLASSICAL SVM | [code](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/SVM/SupportVectorMachine(SVM)/svm.py) | KERNEL SVM [CODE](https://github.com/kennedyCzar/ADVANCE-MACHINE-LEARNING-KERNEL-METHOD/blob/master/SVM/SupportVectorMachine(SVM)/kernelSVM.py)
##### Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. It is a powerful classification algorithm with good theoretical guarantees. 
